\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage[a4paper, total={6.5in, 9in}]{geometry}

\usepackage{natbib}
\bibliographystyle{unsrtnat}

\title{Literature review for the PhD defense of \linebreak Gustavo Quintana-Carapia} 

\begin{document}



\maketitle

There are two main parts in this document. 
First there is an alphabetic list of the contributions described ion the literature.
Later, there is a review of the literature based on the read papers.

\section{Contribution of the papers}


\citet{Alaziz17} improve the during-sleep body motion classification using a load cell based system. With a machine learning algorithm, the authors classify a movement into one of 9 classes by extracting 24 features. The authors found that the method it classified correctly 90 \% of movements, and it is convenient for long-term home monitoring.

\citet{Azam15} propose a  dual implementation of the Kalman filter for estimating the unknown input and states of a linear state-space model by using sparse noisy acceleration measurements. The authors explain that the successive structure of the suggested filter prevents numerical issues attributed to un-observability and rank deficiency of the augmented formulation of the problem. The authors argument that the proposed methodology avoids the drift in the estimated input and displacements that is commonly encountered by existing joint input and state estimation filters. The authors found that the proposed technique obtains, by fine-tuning its parameters, accurate estimates of displacements and velocities.

\citet{Ballo16}  present an innovative six axis load cell for frontal impact tests on a dummy.  The authors measure the acceleration in two different points of the dummy's head, and the impact force input. The authors explain the observations with a first order model of the head and neck, and the neck stiffness and damping have been identified from experimental data.

\citet{Beck10}


\citet{Boschetti13} propose a model-based scheme that compensates for low frequency environmental vibrations in automatic weighing machines made of load cells. The authors think that it is ineffective to use low-pass filtering only in load cell measurements to overcome this problem. Their argument is that low cut-off frequencies downgrade machine speed by both introducing delay and increasing filtered signal rise time. The authors instead use mechanical models of the weighing machine and the load cells to process supplementary accelerometer measurements for computing an effective compensation of the environmental vibrations on load cell response effects.

\citet{Burmen09} present a method for fast and accurate in-line weighing of hard gelatin capsules based on capacitance sensor. The authors studied the effect of the shape and size of the capacitive sensor on the sensitivity and stability of the measurements to optimize the performance of the system. The results show that the capacitance iss well correlated with the capsule weight.

\citet{Cox06} show that a Monte Carlo method is an effective and versatile tool for determining the PDF for the measurands, and it provides a consistent Bayesian approach to the evaluation of uncertainty. The authors provide a guidance on optimizing the approach, identifies some pitfalls and indicates means for validating the results.

\citet{daSilva12} reviews and analyzes studies concerning measurement uncertainty. This paper provides a short summary of the state of the art of measurement uncertainty, analyzes the research scenario, and, suggests future work based on the analysis.

\citet{DEmilia16} recognize the relevance of three-axis accelerometers and inclinometers in buildings diagnostics. The authors first focus on the sensors calibration, including the low frequency range, 0-10 Hz. Later, they minimize systematic errors and identify the factors that affect uncertainty: the motion positioning of the accelerometer with respect to the vertical axis, the imposed motion law, and the radial positioning of the sensor. 

\citet{Dienstfrey14} rederives the heuristic estimation rules that correct the measured quantities for the finite bandwidth of the measurement system response.
The rules are necessary when the measurement device responds slowly compared to the modulation of the input signal. The authors investigate the rules quantitative performance, and as an alternative, demonstrate that regularized deconvolution analysis exhibits more general quantitative utility at the expense of increased measurement burden and analytical complexity.

\citet{Diniz17} present a methodology for estimating the uncertainty of measurements in the calibration of temperature sensors in dynamic regimes for industrial applications. The authors use a first-order bare-wire-beaded thermocouple sensor as a case study. The experimental setup is costructed to give the input as a temperature step. The paper describes a methodology to compute the measurement uncertainty considering the uncertainties of all components of the measurement chain.

\citet{Elster07} analyzes the dynamic measurements using a second-order accelerometer dynamic model, assuming that estimates of the model parameters are available after previous dynamic calibration measurements. The authors want to estimate the unknown input of the sensor given its discrete-time output signal, using an FIR filter constructed from the underlying dynamic model. The input  uncertainties are determined considering the uncertainty of the dynamic model parameters estimates, the uncertainty of the sensor's output signal, and the applied signal processing steps. The proposed method allows real-time estimations.

\citet{Elster08} consider the measurement uncertainty evaluation when the value of the measurand depends on the continuous variable time. The authors generalize the GUM approach to the uncertainty of linear and time-invariant models. The paper describes a digital FIR filter for estimating the time-dependent  measurand value and an example is given for a second-order model.

\citet{Eichstadt16b} explore the relationship between the covariance matrix produced by the Kalman filter, a GUM-compliant uncertainty analysis, and a Bayesian uncertainty analysis. The authors found that all three approaches are compatible for linear systems with known system matrices. However, different results are obtained when the system matrices are not precisely known, or when the system is nonlinear, which might use the extended Kalman filter with linearized uncertainty propagation. The paper shows that the extended Kalman filter linearization and convergence errors can be avoided by applying Monte Carlo.

\citet{Esward16} present four examples of work at the UK's National Physical Laboratory that demonstrate how modelling and simulation can improve the understanding of dynamic measurement tasks. The presented examples are (i) a software simulation of a lock-in amplifier, (ii) a simulation of a sensor network in which one of the sensors has insufficient bandwidth for the measurement task and Kalman filter based data fusion is used to aggregate the sensor outputs, (iii) a study of performance imperfections in a clock embedded in a wireless sensor node using a Monte Carlo based method for simulating counting errors, and (iv) a simulation of wave propagation in a shock tube in which the lattice Boltzmann method was used to study non-ideal behaviour of the shock tube. The authors found that simulations are useful for designing measuring systems and identifying and quantifying measurement uncertainties and that the development of simulation software requires the developer to have a clear understanding of the measuring system of interest.

\citet{Esward09} describes the role and relevance of the GUM in the light of the challenges of new dynamic measurements, where the measurand variation has a significant effect on the measurement result and the associated uncertainty. The authors think that the challenges may require development of new measurement and calibration methods, as well as new methods for correcting sensor outputs and for the evaluation of uncertainties. 

\citet{Eichstadt10} describes a procedure for digital deconvolution filters that reconstruct a time-variable measurand from the measurement signal of a LTI measurement system, exploiting the potentialities of digital signal processing. The authors discuss methods of minimum-phase all pass decomposition, asynchronous time reversal using the exact inverse filter and the construction of stable infinite impulse response and finite impulse response approximate inverse filters by a least squares approach in the frequency domain. The paper concludes that when a continuous model of the LTI system is available, application of least squares in the frequency domain for the construction of an approximate inverse filter is preferred. On the other hand, asynchronous time reversal filtering using the exact inverse filter appears superior when a discrete model of the LTI system is available and when causality of the deconvolution filter is not an issue.

\citet{Feiz17} assume that structured total least squares (STLS) for the fidelity term in color image restoration process gives better results than least squares (LS) but are more complex. The authors transform the color image restoration in two smaller subproblems for the image smooth and oscillatory parts. The authots solve the problems with STLS and LS, respectively. The paper shows that the proposed method is faster than STLS and gives competitive solutions. 

\citet{Ferrero06} review of the present practice for expressing and estimating uncertainty in measurement and reports the  fundamental concepts of measurement science: 1) The result of a measurement provides only incomplete knowledge of the measurand, whose true value remains unknown and unknowable; 2) The uncertainty concept plays a key role in quantifying the incompleteness of the knowledge provided by the measurement result; and 3) A measurement result can be usefully employed only if the associated uncertainty is estimated and if it can be traced back to the appertaining standard; otherwise, it is a meaningless value.


\citet{Guo18} A rule of the effect between the output of a single load cell and a combinatorial structure was proposed by changing the sensitivity of the single load cell. The relevant relationship between the sensitivity of single load cell and the variation of total output of the combinatorial structure was analyzed by the numerical derivation method and experimental validation. In the cases, the output of a single load cell is linear or nonlinear, when the same load was acted on the combinatorial structure. The results showed that the effect was limited and the influence quantity is due to the accuracy and number of load cell. However, it must be controlled in a certain range to avoid additional measurement errors to the combinatorial structure. Moreover, the rule provides an evaluation method with improving the measurement accuracy for the combinatorial structure. Consequently, the rule can be employed as a useful guidance for the product quality when this evaluation method is applied to the load cell selection for the electronic truck scales.

\citet{Hale09} describe a method for calibrating the voltage that a step-like pulse generator produces at a load at every time point in the measured waveform. The calibration includes an equivalent-circuit model of the generator that can be used to determine how the generator behaves when it is connected to arbitrary loads. The generator is calibrated with an equivalent-time sampling oscilloscope and is traceable to fundamental physics via the electro-optic sampling system at the National Institute of Standards and Technology. The calibration includes a covariance-based uncertainty analysis that provides the uncertainty at each time in the waveform vector and the correlations between the uncertainties at the different times. From the calibrated waveform vector and its covariance matrix, we calculate pulse parameters and their uncertainties. We compare our method with a more traditional parameter-based uncertainty analysis.

\citet{Hammersley75} describes the theory and applications of  Monte Carlo Methods

\citet{Hernandez06} Load cells are transducers used to measure force or weight. Despite the fact thatthere is a wide variety of load cells, most of these transducers that are used in the weighingindustry are based on strain gauges. In this paper, an s-beam load cell based on strain gaugeswas suitably assembled to the mechanical structure of several seats of a bus underperformance tests and used to measure the resistance of their mechanical structure to tensionforces applied horizontally to the seats being tested. The load cell was buried in a broad-band noise background where the unwanted information and the relevant signal sometimesshare a very similar frequency spectrum and its performance was improved by using arecursive least-squares (RLS) lattice algorithm. The experimental results are satisfactoryand a significant improvement in the signal-to-noise ratio at the system output of 27 dB wasachieved, which is a good performance factor for judging the quality of the system.

\citet{Hessling06} A general method for estimating an upper bound of the dynamic measurement error in the time domain is derived, assuming that the complex-valued frequency response function of the system is known. This dynamic error bound related to the dynamic response of the system can be included in the measurement uncertainty as an uncorrected systematic error. The only restriction in the analysis is on linearity of the system, which can include electrical filters, mechanical sensors, amplifiers and similar subsystems. Calculations are verified by simulations for two example systems. The results for one of the most commonly used models of force and pressure transducers and accelerometers are presented emphasizing the main differences to a common estimate. For signals with well-confined spectra the linear dynamic error arises for two reasons: varying amplification and delay with frequency. The latter is analogous to the well-known bandwidth limiting dispersion of signals. For extended spectrum signals/simple pulses the asymptotic tails of the spectra may generate the major part of the error. No widespread robust, general and systematic method of quantifying the measurement error caused by these effects exists, despite that they may generate signal distortion far beyond the common prediction

\citet{Hessling08b} presents an approach to dynamic evaluation of measurement systems. On the one hand, it separates physical experiments, analysis and signal processing methods into successive steps of evaluation. On the other hand, the structure allows for resolution of an entire measurement system into its components for dedicated analyses. There is no limitation to particular applications except that it should be possible to model the response of the measurement system with differential equations with constant coefficients. Proposed tools such as estimation of error and uncertainty and direct mapping methods for synthesis of signal restoration filters are new or recently published, while others like system identification are well known but not previously systematically used in the context of calibration.

\citet{Hessling10} Dynamic Metrology is devoted to the analysis of dynamic measurements.  As  an  extended  calibration  service,  it  contains  many  novel  ingredients currently not included in the standard palette of metrology. Rather, Dynamic Metrology encompasses many operations found in the fields of system identification, digital signal processing and control theory. The analyses are more complex and more ambiguous than conventional uncertainty budgets of today. The important interactions in non-stationary measurements  may  be  exceedingly  difficult  to  both  control  and  to  evaluate.  In  many situations, in situ calibrations are required to yield a relevant result. Providing metrological services in this context will be a true challenge

\citet{Hessling08a} The dynamic error of measured signals is sometimes unacceptably large. If the dynamic properties of the measurement system are known, the true physical signal may to some extent be re-constructed. With a parametrized characterization of the system and sampled signals, time-domain digital filters may be utilized for correction. In the present work a general method for synthesizing such correction filters is developed. It maps the dynamic parameters of the measurement system directly on to the filter coefficients and utilizes time reversed filtering. This avoids commonly used numerical optimization in the filter synthesis. The method of correction is simple with absolute repeatability and stability, and results in a low residual error. Explicit criteria to control both the horizontal (time) and vertical (amplitude) discretization errors are presented in terms of the utilization of bandwidth and noise gain, respectively. To evaluate how close to optimal the correction is, these errors are also formulated in relation to the signal-to-noise ratio of the original measurement system. For purposes of illustration, typical mechanical and piezo-electric transducer systems for measuring force, pressure or acceleration are simulated and dynamically corrected with such dedicated digital filters.

\citet{Hessling11} The time-dependent measurement uncertainty has been evaluated in a number of recent publications, starting from a known uncertain dynamic model. This could be defined as the 'downward' propagation of uncertainty from the model to the targeted measurement. The propagation of uncertainty 'upward' from the calibration experiment to a dynamic model traditionally belongs to system identification. The use of different representations (time, frequency, etc) is ubiquitous in dynamic measurement analyses. An expression of uncertainty in dynamic measurements is formulated for the first time in this paper independent of representation, joining upward as well as downward propagation. For applications in metrology, the high quality of the characterization may be prohibitive for any reasonably large and robust model to pass the whiteness test. This test is therefore relaxed by not directly requiring small systematic model errors in comparison to the randomness of the characterization. Instead, the systematic error of the dynamic model is propagated to the uncertainty of the measurand, analogously but differently to how stochastic contributions are propagated. The pass criterion of the model is thereby transferred from the identification to acceptance of the total accumulated uncertainty of the measurand. This increases the relevance of the test of the model as it relates to its final use rather than the quality of the calibration. The propagation of uncertainty hence includes the propagation of systematic model errors. For illustration, the 'upward' propagation of uncertainty is applied to determine if an appliance box is damaged in an earthquake experiment. In this case, relaxation of the whiteness test was required to reach a conclusive result.

\citet{Hessling13a} Deterministic sampling can be used for nonlinear propagation of the statistics of signal processingmodels. Unlike Monte Carlo methods, random generators are not utilized in any stage. The samplesare instead calculated deterministically. Our novel approach generalizes the deterministic samplingtechnique for propagating covariance in the unscented Kalman filter by introducing genericexci-tation matricesdescribing small discrete canonical ensembles. The approximation lies in how wellthe available statistical information is encoded in the discrete ensemble, not how each sample ispropagated. The application and performance of deterministic sampling are illustrated for a typicalstep response analysis of an electrical device modeled with an uncertain digital filter.

\citet{Hessling13b} Statistical signal processing [1] traditionally focuses on extraction of information from noisy measurements. Typically, parameters or states are estimated by various filtering operations. Here, the quality of signal processing operations will be assessed by evaluating the statistical uncertainty of the result [2]. The processing could for instance simulate, correct, modulate, evaluate, or control the response of a physical system. Depending on the addressed task and the system, this can often be formulated in terms of a differential or difference signal processing model equation in time, with uncertain parameters and driven by an exciting input signal corrupted by noise. The quantity of primary interest may not be the output signal but can be extracted from it. If this uncertain dynamic model is linear-in-response it can be translated into a linear digital filter for highly efficient and standardized evaluation [3]. A statistical model of the parameters describing to which degree the dynamic model is known and accurate will be assumed given, instead of being the target of investigation as in system identification [4]. Model uncertainty (of parameters) is then propagated to model-ing uncertainty (of the result). The two are to be clearly distinguished - the former relate to the input while the latter relate to the output of the model. 

\citet{Huang16} An approach to mass measurement for the electronic balance based on continuous-time sigma-delta (CT ?-?) modulator is described in this paper. As an effect of multifarious interfering noises, the accuracy of the electronic balance is restricted. The general idea of this proposed approach is to apply an electromagnetic-force-compensated load cell (EMCC), a related signal processing circuit, and a composite filter. The circuit mainly consists of a proportional-integral-differential controller and a pulsewidth modulator, which combine with the EMCC to form a CT ?-? modulator. As the CT ?-? modulator possesses inherent antialiasing filtering, oversampling, and noise-shaping characteristics, the interfering noises can be effectively removed by the composite filter that is composed of two sinc N filters. The simulations for both of the electronic balance models adopting the proposed approach and working in conventional pulse current mode are analyzed by adding the white Gaussian noise in Simulink. The simulation results demonstrate the effectiveness of this proposed approach, the proposed electronic balance in the field can improve its signal-to-noise ratio, and the testing results meet the requirement for the weighing accuracy of the special-class scale defined by Organisation Internationale De M\'etrologie L\'egale R76 nonautomatic weighing instruments.

\citet{Jafaripanah05}  investigate the application of analog adaptive techniques to the area of dynamic sensor compensation, of which there is little reported work in the literature. The case is illustrated by showing how the response of a load cell can be improved to speed up the process of measurement. The load cell is a sensor with an oscillatory response in which the measurand contributes to the response parameters. Thus, a compensation filter needs to track variation in measurand, whereas a simple fixed filter is only valid at one specific load value. To facilitate this investigation, computer models for the load cell and the adaptive compensation filter have been developed. To allow a practical implementation of the adaptive techniques, a novel piecewise linearization technique is proposed in order to vary a floating voltage-controlled resistor in a linear manner over a wide range. Simulation and practical results are presented, thus, demonstrating the effectiveness of the proposed techniques.

\citet{Jia18} In this paper, we focus on the target localization problem which finds broad applications in radar, sonar and wireless sensor networks. A pseudolinear overdetermined system of equations is constructed from the nonlinear hybrid TDOA-AOA measurements about target location. Considering the matrix and vector in the constructed pseudolinear system are both contaminated by the measurement noise, a new weight least squares (WLS) method which is based on the first order Taylor expansions of the noise terms is developed in this paper and it can reduce the estimation bias that arise from the least squares (LS) method. In particular we focus on constructing a localization algorithm to reduce the bias that easily arise from the traditional methods. Thus in addition, a novel structured total least squares (STLS) method is also developed in this paper to further reduce the estimation bias specially when the target is outside the convex hull formed by sensors. Numerical examples show the superiority of the proposed STLS method in estimation accuracy compared with the LS method, total least squares (TLS) method and the proposed WLS method.


\citet{Kesilmis16} We propose a novel geometric approach to processing of load cell signals for inline weight measurement applications. These systems usually rely on oscillatory load cell signals which always need to time to settle. We describe the theoretical and experimental implementation of a newly proposed geometric approach for load cell signals, and verify the effectiveness of the method through experiments.

\citet{Kiviet12high} An approximation to order is obtained for the bias of the full vector of least-squares estimates obtained from a sample of size in general stable but not necessarily stationary ARX(1) models with normal disturbances. This yields generalizations, allowing for various forms of initial conditions, of Kendall's and White's classic results for stationary AR(1) models. The accuracy of various alternative approximations is examined and compared by simulation for particular parameterizations of AR(1) and ARX(1) models. The results show that often the second-order approximation is considerably better than its first-order counterpart and hence opens up perspectives for improved bias correction. However, order approximations are also found to be more vulnerable in the near unit root case than the much simpler order approximations.

\citet{Kiviet14improved} In dynamic regression models conditional maximum likelihood (least-squares) coefficient and variance estimators are biased. Using expansion techniques an approximation is obtained to the bias in variance estimation yielding a bias corrected variance estimator. This is achieved for both the standard and a bias corrected coefficient estimator enabling a comparison of their mean squared errors to second order. Sufficient conditions for admissibility of these approximations are formally derived. Illustrative numerical and simulation results are presented on bias reduction of coefficient and variance estimation for three relevant classes of first-order autoregressive models, supplemented by effects on mean squared errors, test size and size corrected power. These indicate that substantial biases do occur in moderately large samples, but these can be mitigated considerably and may also yield mean squared error reduction. Crude asymptotic tests are cursed by huge size distortions. However, operational bias corrections of both the estimates of coefficients and their estimated variance (for which software is provided) are shown to curb type I errors reasonably well.


\citet{Lee16} Ballistocardiographs (BCGs), which record the mechanical activity of the heart, have been a subject of interest for several years because of their advantages in providing unobtrusive physiological measurements. BCGs could also be useful for monitoring the biological signals of infants without the need for physical confinement. In this study, we describe a physiological signal monitoring bed based on load cells and assess an algorithm to extract the heart rate and breathing rate from the measured load-cell signals. Four infants participated in a total of 13 experiments. As a reference signal, electrocardiogram and respiration signals were simultaneously measured using a commercial device. The proposed automatic algorithm then selected the optimal sensor from which to estimate the heartbeat and respiration information. The results from the load-cell sensor signals were compared with those of the reference signals, and the heartbeat and respiration information were found to have average performance errors of 2.55% and 2.66%, respectively. The experimental results verify the positive feasibility of BCG-based measurements in infants.

\citet{Lemmerling02} In this paper an overview is given of the Structured Total Least Squares (STLS) approach and its recent extensions. The Structured Total Least Squares (STLS) problem is a natural extension of the Total Least Squares (TLS) problem when constraints on the matrix structure need to be imposed. Similar to the ordinary TLS approach, the STLS approach can be used to determine the parameter vector of a linear model, given some noisy measurements. In many signal processing applications, the imposition of this matrix structure constraint is necessary for obtaining Maximum Likelihood (ML) estimates of the parameter vector.

\citet{Link07} A recently proposed accelerometer model is applied for determining the accelerometer's output to transient accelerations. The model consists of a linear, second-order differential equation with unknown coefficients. It is proposed to estimate these model parameters from sinusoidal calibration measurements, and an estimation procedure based on linear least-squares is presented. In addition, the uncertainties associated with the estimated parameters are determined utilizing a Monte Carlo simulation technique.

The performance of the proposed modelling approach was tested by its application to calibration measurements of two back-to-back accelerometers (ENDEVCO type 2270 and Br{\"u}el \& Kj{\ae}r type 8305). For each of the two accelerometers, the model was first estimated from sinusoidal calibration measurements and then used to predict the accelerometer's behaviour for two shock calibration measurements. Measured and predicted shock sensitivities were found consistent with differences below 1% in most cases which confirms the benefit of the proposed modelling approach.

\citet{Link09} propose a method  for evaluating the uncertainty associated with the output of a discrete-time IIR filter when the input signal is corrupted by additive noise and the filter coefficients are uncertain. This task arises, for instance, when the noise-corrupted output of a measurement system is compensated by a digital filter which has been designed on the basis of the characteristics of the measurement system. We assume that the noise is either stationary or uncorrelated, and we presume knowledge about its autocovariance function or its time-dependent variances, respectively. Uncertainty evaluation is considered in line with the 'Guide to the Expression of Uncertainty in Measurement'. A state-space representation is used to derive a calculation scheme which allows the uncertainties to be evaluated in an easy way and also enables real-time applications. The proposed procedure is illustrated by an example.

\citet{Markovsky07overview} review the development and extensions of the classical total least-squares method and describe algorithms for its generalization to weighted and structured approximation problems. In the generic case, the classical total least-squares problem has a unique solution, which is given in analytic form in terms of the singular value decomposition of the data matrix. The weighted and structured total least-squares problems have no such analytic solution and are currently solved numerically by local optimization methods. We explain how special structure of the weight matrix and the data matrix can be exploited for efficient cost function and first derivative computation. This allows to obtain computationally efficient solution methods. The total least-squares family of methods has a wide range of applications in system theory, signal processing, and computer algebra. We describe the applications for deconvolution, linear prediction, and errors-in-variables system identification.

\citet{Markovsky15cep} Metrology is advancing by development of new measurement techniques and corresponding hardware. A given measurement technique, however, has fundamental speed and precision limitations. In order to overcome the hardware limitations, we develop signal processing methods based on the prior knowledge that the measurement process dynamics is linear time-invariant.
Our approach is to model the measurement process as a step response of a dynamical system, where the input step level is the quantity of interest. The solution proposed is an algorithm that does real-time processing of the sensor's measurements. It is shown that when the measurement process dynamics is known, the input estimation problem is equivalent to state estimation. Otherwise, the input estimation problem can be solved as a system identification problem. The main underlying assumption is that the measured quantity is constant and the measurement process is a low-order linear time-invariant system. The methods are validated and compared on applications of temperature and weight measurement.

\citet{Markovsky15ieee} Dynamic measurement aims to improve the speed and accuracy characteristics of measurement devices by signal processing. State-of-the-art dynamic measurement methods are model-based adaptive methods, i.e., 1) they estimate model parameters in real-time and 2) based on the identified model perform model-based signal processing. The proposed model-free method belongs to the class of the subspace identification methods. It computes directly the quantity of interest without an explicit parameter estimation. This allows efficient computation as well as applicability to general high order multivariable processes.



\citet{Matthews14}  focus on the mathematical modelling required to support the development of new primary standard systems for traceable calibration of dynamic pressure sensors. We address two fundamentally different approaches to realizing primary standards, specifically the shock tube method and the drop-weight method. Focusing on the shock tube method, the paper presents first results of system identification and discusses future experimental work that is required to improve the mathematical and statistical models.
The authors use simulations to identify differences between the shock tube and drop-weight methods, to investigate sources of uncertainty in the system identification process and to assist experimentalists in designing the required measuring systems. We demonstrate the identification method on experimental results and draw conclusions.

\citet{Mayne14} reviews Model Predictive Control, gives an overview of some current developments and suggests a few avenues for future research.

\citet{Neuberger07}

\citet{Niedzwiecki16a} Dynamic weighing, i.e., weighing of objects in motion, without stopping them on the weighing platform, allows one to increase the rate of operation of automatic weighing systems, used in industrial production processes, without compromising their accuracy. Since the classical identification-based approach to dynamic weighing, based on the second-order mass-spring-damper model of the weighing system, does not yield satisfactory results when applied to conveyor belt type checkweighers, several extensions of this technique are examined. Experiments confirm that when appropriately modified the identification-based approach becomes a reliable tool for dynamic mass measurement in checkweighers.

\citet{Niedzwiecki16b} propose to characterize as a filtering scheme based on the finite impulse response model of the weighing system response. It is shown that when such a model-based filtering is applied, the attained weighing accuracy is up to four times higher than that guaranteed by the currently available state-of-the-art solutions.
Conveyor belt-type checkweighers are increasingly popular components of modern production lines. They are used to assess the weight of the produced items in motion, i.e., without stopping them on the weighing platform. The main challenge one faces when designing a dynamic weighing system is providing high measurement accuracy, especially at high conveyor belt speeds. 

\citet{Ogorevc16}  describe a procedure for dynamic measurement uncertainty evaluation in order to examine the requirements for clinical thermometer dynamic properties in standards and recommendations. In this study thermistors were used as temperature sensors, transient temperature measurements were performed in water and air and the measurement data were processed for the investigation of thermometer dynamic properties. The thermometers were mathematically modelled. A Monte Carlo method was implemented for dynamic measurement uncertainty evaluation. The measurement uncertainty was analysed for static and dynamic conditions. Results showed that dynamic uncertainty is much larger than steady-state uncertainty. The results of dynamic uncertainty analysis were applied on an example of clinical measurements and were compared to current requirements in ISO standard for clinical thermometers. It can be concluded that there was no need for dynamic evaluation of clinical thermometers for continuous measurement, while dynamic measurement uncertainty was within the demands of target uncertainty. Whereas in the case of intermittent predictive thermometers, the thermometer dynamic properties had a significant impact on the measurement result. Estimation of dynamic uncertainty is crucial for the assurance of traceable and comparable measurements.
Clinical thermometers in intensive care units are used for the continuous measurement of body temperature. 

\citet{OIML_R51_1} specifies  the  metrological  and  technical  requirements  and  test  procedures for automatic catchweighing instruments (catchweighers), hereinafter called "instruments", that are subject to national metrological control. 

\citet{Olmi16} present a didactic project at the University of Bologna, focused on the development of strain gage load cells, from design to calibration and use. Details are provided on the course units involved and on the levels of teaching, and three case studies are presented. Two case studies deal with the design of decoupled load cells, using simple beam geometry. Students had the opportunity to become confident with the principles ofmechanics and to tackle the not trivial activities like calibration by decoupled load application and determination of the related matrices. The third one deals with the development of a load cell for impact loads, where the students had the chance to observe a practical application of the studied Wheatstone bridge connection, to participate in on-field tests, and to better understand the close relationship between experimentation and design.
Many numerical models are nowadays available for the structural analysis of complexly shaped structures. However, a critical problem consists of the estimation of the actual loads that a structure withstands. The most proper way to determine them under service conditions consists of executing in-field tests, where load cells have an important role. This article deals

\citet{Palanthandalam10parameter}  investigate the consistency of parameter estimates obtained from least-squares identification with a quadratic parameter constraint. For generality, we consider infinite impulse-response systems with coloured input and output noise. In the case of finite data, we show that there always exists a possibly indefinite quadratic constraint depending on the noise realisation that results in a constrained optimisation problem that yields the true parameters of the system when a persistency condition is satisfied. When the noise covariance matrix is known to within a scalar multiple, we prove that solutions of the quadratically constrained least-squares (QCLs) estimator with a semidefinite constraint matrix are both unbiased and consistent in the sense that the averaged problem and limiting problem produce, respectively, unbiased and true (with probability 1) estimators. In addition, we provide numerical results that illustrate these properties of the QCLS estimator.

\citet{Pintelon90} present a method for compensating in real time for a nonideal transfer function of a data acquisition channel by means of a digital IIR (infinite impulse response) filter. Real-time compensation of the amplitude and phase characteristics of an acquisition channel so that the long-term error is less than +or-0.01 dB and +or-0.1 degrees , respectively, has been obtained experimentally. It is shown that stability and linearity are the main requirements for the components of the data acquisition channel. Careful selection of the components and outlining of the antialias filter and the programmable amplifier is no longer necessary.


\citet{Pietrzak14}  presents and verifies experimentally a new approach, based on time-variant low-pass filtering, to overcome classical linear time-invariant low-pass filtering results for high conveyor belt speeds. The authors show that, when properly tuned, the proposed time-variant filter fulfills the measurement accuracy requirements for a wide range of operating conditions.
Conveyor belt type checkweighers are complex mechanical systems consisting of a weighing sensor (strain gauge load cell, electrodynamically compensated load cell), packages (of different shapes, made of different materials) and a transport system (motors, gears, rollers). Disturbances generated by the vibrating parts of such a system are reflected in the signal power spectra in a form of strong spectral peaks, located usually in the lower frequency range. Such low frequency components overlap in the frequency domain with the useful signal and it is very difficult to eliminate them. The conventional way of suppressing disturbances is via low-pass filtering of the signal obtained from the load cell. However, if the speed of the conveyor belt is high, the response of the applied filter may not settle fast enough to enable accurate weighing of objects in motion, i.e., without stopping them on the weighing conveyor.

\citet{Piskorowski08} presents a new method for dynamic compensation of the load cell response using linear time-varying continuous-time filter. Load cells have an oscillatory response which always needs time to settle down. Therefore, it is justified to search a technique for an effective improvement of the sensor response. The paper describes a theoretical implementation of the proposed time-varying filter, and suggests the implementation technique with the aid of which this kind of filter can be implemented in practice. Simulation results verifying the effectiveness of the proposed filter are presented and compared to the traditional time-invariant configuration.


\citet{Rhode14recursive} show that the generalized total least squares (GTLS) problem with a singular noise covariance matrix is equivalent to the restricted total least squares (RTLS) problem and propose a recursive method for its numerical solution. The method is based on the generalized inverse iteration. The estimation error covariance matrix and the estimated augmented correction are also characterized and computed recursively. The algorithm is cheap to compute and is suitable for online implementation. Simulation results in least squares (LS), data least squares (DLS), total least squares (TLS), and restricted total least squares (RTLS) noise scenarios show fast convergence of the parameter estimates to their optimal values obtained by corresponding batch algorithms.

\citet{Rossander15} measures unique blade force on an open site straight-bladed vertical axis wind turbine. This paper presents a method for measuring the tangential and normal forces on a 12-kW vertical axis wind turbine prototype with a three-bladed H-rotor. Four single-axis load cells were installed in-between the hub and the support arms on one of the blades. The experimental setup, the measurement principle, together with the necessary control and measurement system are described. The maximum errors of the forces and accompanying weather data that can be obtained with the system are carefully estimated. Measured forces from the four load cells are presented, as well as the normal and tangential forces derived from them and a comparison with theoretical data. The measured torque and bending moment are also provided. The influence of the load cells on the turbine dynamics has also been evaluated. For the aerodynamic normal force, the system provides periodic data in agreement with simulations. Unexpected mechanical oscillations are present in the tangential force, introduced by the turbine dynamics. The measurement errors are of an acceptable size and often depend on the measured variable. Equations are presented for the calculation of measurement errors

\citet{Saggin01} studies the dynamic correction of a thermometer for atmospheric measurements. The sensor analysed is the one launched onboard the Huygens probe of the Cassini mission. The specific aspects of the measurement are related to the strong changes in the fluid-dynamic conditions during the operative phase. This peculiarity does not allow the use of one of the various correction techniques based on the knowledge of the dynamic characteristics of the sensor. The thermometer, however, has the peculiarity of being composed of two independent platinum sensors, having very different dynamic properties. The study, starting from a previously conceived dynamic scheme of the sensor, assesses the possibility of determining continuously, from measurement data, the current values of the four time constants required for dynamic characterisation. Lastly one procedure for the dynamic correction of the measurements is presented, along with some verification of its effectiveness and, of the impact on the overall measurement uncertainty.

\citet{Shu93} derive a discrete output error (OE) model of second order for the weighing system dynamics. Using the model and the recursive least squares (RLS) procedure, model parameters and then the mass being weighed can be estimated from a dynamic measurement signal of very short duration. The validity and the accuracy of this method are illustrated by digital simulation studies and real-life measurements

\citet{Soderstrom07} gives a survey of errors-in-variables methods in system identification. Background and motivation are given, and examples illustrate why the identification problem can be difficult. Under general weak assumptions, the systems are not identifiable, but can be parameterized using one degree-of-freedom. Examples where identifiability is achieved under additional assumptions are also provided. A number of approaches for parameter estimation of errors-in-variables models are presented. The underlying assumptions and principles for each approach are highlighted.

\citet{Soderstrom18} gives a comprehensive overview of errors-in-variables (EIV) problems in system identification. This problem is about modeling of dynamic systems when all measured variables and signals are noise-corrupted. A number of different approaches are described and analyzed. Representations where errors or measurement noises are present on both inputs and outputs are usually called 'errors-in-variables' (EIV) models. They play an important role when the purpose is the determination of the physical laws that describe the process, rather than the prediction of its future behavior.

\citet{Stewart90SPT} approaches  classical matrix perturbation theory from a probabilistic point of view. The perturbed quantity is approximated by a first-order perturbation expansion, in which the perturbation is assumed to be random. This permits the computation of statistics estimating the variation in the perturbed quantity. Up to the higher-order terms that are ignored in the expansion, these statistics tend to be more realistic than perturbation bounds obtained in terms of norms. The technique is applied to a number of problems in matrix perturbation theory, including least squares and the eigenvalue problem.

\citet{Tasaki07} uses a multi-stage conveyor belt scale to propose a simplified and effective mass estimation algorithm under practical vibration modes. The output signals from the conveyor belt scales are always contaminated with noises due to vibrations of the conveyor belt and the product in motion. In this paper digital filter of finite-duration impulse response (FEB) type is designed to provide adequate accuracy. The experimental results on conveyor belt scales suggest that the filtering algorithm proposed here is effective enough to practical applications. As long as spaces between successive products are set within a specified range, the products can be weighed correctly even if products having different lengths are transported in random manner.
Today higher speed of operation and highly accurate weighing of packages during crossing a conveyor belt has been getting more important in the food and distribution industries etc. Continuous weighing means that masses of discrete packages on a conveyor belt are automatically determined in sequence. 

\citet{Vaccaro94} develops a perturbation expansion for the subspace spanned by a set of singular vectors. A first-order expansion of this type has recently been developed and used to analyze the performance of direction-finding algorithms in array signal processing. The author derives a new second-order expansion and the result is illustrated with two examples.

\citet{VanHuffel91Book}  give a unified presentation of the TLS problem. A description of its basic principles are given, the various algebraic, statistical and sensitivity properties of the problem are discussed, and generalizations are presented. The TLS applications are surveyed to facilitate uses in an even wider range of applications. The authors provide comparison  with the well-known least squares methods.

\citet{VanHuffel07TLSeditorial} arranged a special issue to present an overview of the progress of total least squares (TLS) in computational mathematics and engineering, and as errors-in-variables (EIV) modeling or orthogonal regression in the statistical community. The TLS method is one of several linear parameter estimation techniques that has been devised to compensate for data errors. For set of given multidimensional data points, the idea is to modify all data points in such a way that some norm of the modification is minimized subject to the constraint that the modified vectors satisfy a linear relation. One of the main reasons for TLS popularity is the availability of efficient and numerically robust algorithms in which the singular value decomposition (SVD) plays a prominent role, and that TLS is an application oriented procedure.

\citet{Vlajic16}  describes an apparatus for traceable, dynamic calibration of force transducers using harmonic excitation. In this system, the force applied to the transducer is produced by the acceleration of an attached mass. The capabilities of this system are demonstrated by performing dynamic calibrations of two shear-web-type force transducers up to a frequency of 2?kHz, with an expanded uncertainty below 1.2 \%. Tha authors give an account of all significant sources of uncertainty, including a detailed consideration of the effects of dynamic tilting (rocking), which is a leading source of uncertainty in such harmonic force calibration systems.

\citet{Yamani18} studies the check-weigher that uses a load cell conveying the mass of an object to be measured. In check-weigher it is necessary to estimate the mass value before the scale is balance still in order to high performance. The conveying motor affects the measured value of the scale and the measuring time. The paper describes a dummy conveyor giving the mass of the motor as unbalanced load. The authors found, after static and dynamic analysis, that the unbalanced load is major factor to deteriorate the measurement accuracy of mass.

\citet{Yeredor04homogeneous} shows that the use of some commonly applied constraints, such as a quadratic constraint, can lead to inconsistent estimates of a when solving  "homogeneous least-squares" models of the form Ya?0. These problems are encountered, e.g., when modeling auto-regressive (AR) processes. The authors explain this discrepancy, and give the remedy. The paper discussees numerically appealing solutions.

\citet{Zahradka18} demonstrates the feasibility of distinguishing certain positions and movements with load cell sensors. The paper shows that identification of positions and movements for individuals in bed can be used as a tool in a variety of clinical settings. Six positions were classified with an accuracy of 74.9 \% and six movements were classified with an accuracy of 79.7 \%. Stationary positions were characterized by the signals from the four load cells and the coordinates of the center of mass. Movements were characterized by the changes in load cell signals. 







\section{Literature relevant for Statistical analysis state of the art}

\citet{Cox06} show that a Monte Carlo method is an effective and versatile tool for determining the PDF for the measurands, and it provides a consistent Bayesian approach to the evaluation of uncertainty. The authors provide a guidance on optimizing the approach, identifies some pitfalls and indicates means for validating the results.

\citet{daSilva12} reviews and analyzes studies concerning measurement uncertainty. This paper provides a short summary of the state of the art of measurement uncertainty, analyzes the research scenario, and, suggests future work based on the analysis.

\citet{Dienstfrey14} rederives the heuristic estimation rules that correct the measured quantities for the finite bandwidth of the measurement system response.
The rules are necessary when the measurement device responds slowly compared to the modulation of the input signal. The authors investigate the rules quantitative performance, and as an alternative, demonstrate that regularized deconvolution analysis exhibits more general quantitative utility at the expense of increased measurement burden and analytical complexity.


\citet{Elster07} analyzes the dynamic measurements using a second-order accelerometer dynamic model, assuming that estimates of the model parameters are available after previous dynamic calibration measurements. The authors want to estimate the unknown input of the sensor given its discrete-time output signal, using an FIR filter constructed from the underlying dynamic model. The input  uncertainties are determined considering the uncertainty of the dynamic model parameters estimates, the uncertainty of the sensor's output signal, and the applied signal processing steps. The proposed method allows real-time estimations.

\citet{Elster08} consider the measurement uncertainty evaluation when the value of the measurand depends on the continuous variable time. The authors generalize the GUM approach to the uncertainty of linear and time-invariant models. The paper describes a digital FIR filter for estimating the time-dependent  measurand value and an example is given for a second-order model.

\citet{Eichstadt16b} explore the relationship between the covariance matrix produced by the Kalman filter, a GUM-compliant uncertainty analysis, and a Bayesian uncertainty analysis. The authors found that all three approaches are compatible for linear systems with known system matrices. However, different results are obtained when the system matrices are not precisely known, or when the system is nonlinear, which might use the extended Kalman filter with linearized uncertainty propagation. The paper shows that the extended Kalman filter linearization and convergence errors can be avoided by applying Monte Carlo.

\citet{Esward16} present four examples of work at the UK's National Physical Laboratory that demonstrate how modelling and simulation can improve the understanding of dynamic measurement tasks. The presented examples are (i) a software simulation of a lock-in amplifier, (ii) a simulation of a sensor network in which one of the sensors has insufficient bandwidth for the measurement task and Kalman filter based data fusion is used to aggregate the sensor outputs, (iii) a study of performance imperfections in a clock embedded in a wireless sensor node using a Monte Carlo based method for simulating counting errors, and (iv) a simulation of wave propagation in a shock tube in which the lattice Boltzmann method was used to study non-ideal behaviour of the shock tube. The authors found that simulations are useful for designing measuring systems and identifying and quantifying measurement uncertainties and that the development of simulation software requires the developer to have a clear understanding of the measuring system of interest.

\citet{Eichstadt10} describes a procedure for digital deconvolution filters that reconstruct a time-variable measurand from the measurement signal of a LTI measurement system, exploiting the potentialities of digital signal processing. The authors discuss methods of minimum-phase all pass decomposition, asynchronous time reversal using the exact inverse filter and the construction of stable infinite impulse response and finite impulse response approximate inverse filters by a least squares approach in the frequency domain. The paper concludes that when a continuous model of the LTI system is available, application of least squares in the frequency domain for the construction of an approximate inverse filter is preferred. On the other hand, asynchronous time reversal filtering using the exact inverse filter appears superior when a discrete model of the LTI system is available and when causality of the deconvolution filter is not an issue.

\citet{Feiz17} assume that structured total least squares (STLS) for the fidelity term in color image restoration process gives better results than least squares (LS) but are more complex. The authors transform the color image restoration in two smaller subproblems for the image smooth and oscillatory parts. The authots solve the problems with STLS and LS, respectively. The paper shows that the proposed method is faster than STLS and gives competitive solutions. 

\citet{Ferrero06} review of the present practice for expressing and estimating uncertainty in measurement and reports the  fundamental concepts of measurement science: 1) The result of a measurement provides only incomplete knowledge of the measurand, whose true value remains unknown and unknowable; 2) The uncertainty concept plays a key role in quantifying the incompleteness of the knowledge provided by the measurement result; and 3) A measurement result can be usefully employed only if the associated uncertainty is estimated and if it can be traced back to the appertaining standard; otherwise, it is a meaningless value.

\citet{Hammersley75} describes the theory and applications of  Monte Carlo Methods

\citet{Hessling08a} The dynamic error of measured signals is sometimes unacceptably large. If the dynamic properties of the measurement system are known, the true physical signal may to some extent be re-constructed. With a parametrized characterization of the system and sampled signals, time-domain digital filters may be utilized for correction. In the present work a general method for synthesizing such correction filters is developed. It maps the dynamic parameters of the measurement system directly on to the filter coefficients and utilizes time reversed filtering. This avoids commonly used numerical optimization in the filter synthesis. The method of correction is simple with absolute repeatability and stability, and results in a low residual error. Explicit criteria to control both the horizontal (time) and vertical (amplitude) discretization errors are presented in terms of the utilization of bandwidth and noise gain, respectively. To evaluate how close to optimal the correction is, these errors are also formulated in relation to the signal-to-noise ratio of the original measurement system. For purposes of illustration, typical mechanical and piezo-electric transducer systems for measuring force, pressure or acceleration are simulated and dynamically corrected with such dedicated digital filters.

\citet{Hessling11} The time-dependent measurement uncertainty has been evaluated in a number of recent publications, starting from a known uncertain dynamic model. This could be defined as the 'downward' propagation of uncertainty from the model to the targeted measurement. The propagation of uncertainty 'upward' from the calibration experiment to a dynamic model traditionally belongs to system identification. The use of different representations (time, frequency, etc) is ubiquitous in dynamic measurement analyses. An expression of uncertainty in dynamic measurements is formulated for the first time in this paper independent of representation, joining upward as well as downward propagation. For applications in metrology, the high quality of the characterization may be prohibitive for any reasonably large and robust model to pass the whiteness test. This test is therefore relaxed by not directly requiring small systematic model errors in comparison to the randomness of the characterization. Instead, the systematic error of the dynamic model is propagated to the uncertainty of the measurand, analogously but differently to how stochastic contributions are propagated. The pass criterion of the model is thereby transferred from the identification to acceptance of the total accumulated uncertainty of the measurand. This increases the relevance of the test of the model as it relates to its final use rather than the quality of the calibration. The propagation of uncertainty hence includes the propagation of systematic model errors. For illustration, the 'upward' propagation of uncertainty is applied to determine if an appliance box is damaged in an earthquake experiment. In this case, relaxation of the whiteness test was required to reach a conclusive result.

\citet{Hessling13a} Deterministic sampling can be used for nonlinear propagation of the statistics of signal processingmodels. Unlike Monte Carlo methods, random generators are not utilized in any stage. The samplesare instead calculated deterministically. Our novel approach generalizes the deterministic samplingtechnique for propagating covariance in the unscented Kalman filter by introducing genericexci-tation matricesdescribing small discrete canonical ensembles. The approximation lies in how wellthe available statistical information is encoded in the discrete ensemble, not how each sample ispropagated. The application and performance of deterministic sampling are illustrated for a typicalstep response analysis of an electrical device modeled with an uncertain digital filter.

\citet{Hessling13b} Statistical signal processing [1] traditionally focuses on extraction of information from noisy measurements. Typically, parameters or states are estimated by various filtering operations. Here, the quality of signal processing operations will be assessed by evaluating the statistical uncertainty of the result [2]. The processing could for instance simulate, correct, modulate, evaluate, or control the response of a physical system. Depending on the addressed task and the system, this can often be formulated in terms of a differential or difference signal processing model equation in time, with uncertain parameters and driven by an exciting input signal corrupted by noise. The quantity of primary interest may not be the output signal but can be extracted from it. If this uncertain dynamic model is linear-in-response it can be translated into a linear digital filter for highly efficient and standardized evaluation [3]. A statistical model of the parameters describing to which degree the dynamic model is known and accurate will be assumed given, instead of being the target of investigation as in system identification [4]. Model uncertainty (of parameters) is then propagated to model-ing uncertainty (of the result). The two are to be clearly distinguished - the former relate to the input while the latter relate to the output of the model. 

\citet{Kiviet14improved} In dynamic regression models conditional maximum likelihood (least-squares) coefficient and variance estimators are biased. Using expansion techniques an approximation is obtained to the bias in variance estimation yielding a bias corrected variance estimator. This is achieved for both the standard and a bias corrected coefficient estimator enabling a comparison of their mean squared errors to second order. Sufficient conditions for admissibility of these approximations are formally derived. Illustrative numerical and simulation results are presented on bias reduction of coefficient and variance estimation for three relevant classes of first-order autoregressive models, supplemented by effects on mean squared errors, test size and size corrected power. These indicate that substantial biases do occur in moderately large samples, but these can be mitigated considerably and may also yield mean squared error reduction. Crude asymptotic tests are cursed by huge size distortions. However, operational bias corrections of both the estimates of coefficients and their estimated variance (for which software is provided) are shown to curb type I errors reasonably well.

\citet{Link07} A recently proposed accelerometer model is applied for determining the accelerometer's output to transient accelerations. The model consists of a linear, second-order differential equation with unknown coefficients. It is proposed to estimate these model parameters from sinusoidal calibration measurements, and an estimation procedure based on linear least-squares is presented. In addition, the uncertainties associated with the estimated parameters are determined utilizing a Monte Carlo simulation technique.

\citet{Markovsky07overview} review the development and extensions of the classical total least-squares method and describe algorithms for its generalization to weighted and structured approximation problems. In the generic case, the classical total least-squares problem has a unique solution, which is given in analytic form in terms of the singular value decomposition of the data matrix. The weighted and structured total least-squares problems have no such analytic solution and are currently solved numerically by local optimization methods. We explain how special structure of the weight matrix and the data matrix can be exploited for efficient cost function and first derivative computation. This allows to obtain computationally efficient solution methods. The total least-squares family of methods has a wide range of applications in system theory, signal processing, and computer algebra. We describe the applications for deconvolution, linear prediction, and errors-in-variables system identification.

\citet{Markovsky15cep} Metrology is advancing by development of new measurement techniques and corresponding hardware. A given measurement technique, however, has fundamental speed and precision limitations. In order to overcome the hardware limitations, we develop signal processing methods based on the prior knowledge that the measurement process dynamics is linear time-invariant.
Our approach is to model the measurement process as a step response of a dynamical system, where the input step level is the quantity of interest. The solution proposed is an algorithm that does real-time processing of the sensor's measurements. It is shown that when the measurement process dynamics is known, the input estimation problem is equivalent to state estimation. Otherwise, the input estimation problem can be solved as a system identification problem. The main underlying assumption is that the measured quantity is constant and the measurement process is a low-order linear time-invariant system. The methods are validated and compared on applications of temperature and weight measurement.

\citet{Markovsky15ieee}Dynamic measurement aims to improve the speed and accuracy characteristics of measurement devices by signal processing. State-of-the-art dynamic measurement methods are model-based adaptive methods, i.e., 1) they estimate model parameters in real-time and 2) based on the identified model perform model-based signal processing. The proposed model-free method belongs to the class of the subspace identification methods. It computes directly the quantity of interest without an explicit parameter estimation. This allows efficient computation as well as applicability to general high order multivariable processes.

\citet{Ogorevc16}  describe a procedure for dynamic measurement uncertainty evaluation in order to examine the requirements for clinical thermometer dynamic properties in standards and recommendations. In this study thermistors were used as temperature sensors, transient temperature measurements were performed in water and air and the measurement data were processed for the investigation of thermometer dynamic properties. The thermometers were mathematically modelled. A Monte Carlo method was implemented for dynamic measurement uncertainty evaluation. The measurement uncertainty was analysed for static and dynamic conditions. Results showed that dynamic uncertainty is much larger than steady-state uncertainty. The results of dynamic uncertainty analysis were applied on an example of clinical measurements and were compared to current requirements in ISO standard for clinical thermometers. It can be concluded that there was no need for dynamic evaluation of clinical thermometers for continuous measurement, while dynamic measurement uncertainty was within the demands of target uncertainty. Whereas in the case of intermittent predictive thermometers, the thermometer dynamic properties had a significant impact on the measurement result. Estimation of dynamic uncertainty is crucial for the assurance of traceable and comparable measurements.
Clinical thermometers in intensive care units are used for the continuous measurement of body temperature. 

\citet{Palanthandalam10parameter}  investigate the consistency of parameter estimates obtained from least-squares identification with a quadratic parameter constraint. For generality, we consider infinite impulse-response systems with coloured input and output noise. In the case of finite data, we show that there always exists a possibly indefinite quadratic constraint depending on the noise realisation that results in a constrained optimisation problem that yields the true parameters of the system when a persistency condition is satisfied. When the noise covariance matrix is known to within a scalar multiple, we prove that solutions of the quadratically constrained least-squares (QCLs) estimator with a semidefinite constraint matrix are both unbiased and consistent in the sense that the averaged problem and limiting problem produce, respectively, unbiased and true (with probability 1) estimators. In addition, we provide numerical results that illustrate these properties of the QCLS estimator.

\citet{Rhode14recursive} show that the generalized total least squares (GTLS) problem with a singular noise covariance matrix is equivalent to the restricted total least squares (RTLS) problem and propose a recursive method for its numerical solution. The method is based on the generalized inverse iteration. The estimation error covariance matrix and the estimated augmented correction are also characterized and computed recursively. The algorithm is cheap to compute and is suitable for online implementation. Simulation results in least squares (LS), data least squares (DLS), total least squares (TLS), and restricted total least squares (RTLS) noise scenarios show fast convergence of the parameter estimates to their optimal values obtained by corresponding batch algorithms.

\citet{Soderstrom07} gives a survey of errors-in-variables methods in system identification. Background and motivation are given, and examples illustrate why the identification problem can be difficult. Under general weak assumptions, the systems are not identifiable, but can be parameterized using one degree-of-freedom. Examples where identifiability is achieved under additional assumptions are also provided. A number of approaches for parameter estimation of errors-in-variables models are presented. The underlying assumptions and principles for each approach are highlighted.

\citet{Soderstrom18} gives a comprehensive overview of errors-in-variables (EIV) problems in system identification. This problem is about modeling of dynamic systems when all measured variables and signals are noise-corrupted. A number of different approaches are described and analyzed. Representations where errors or measurement noises are present on both inputs and outputs are usually called 'errors-in-variables' (EIV) models. They play an important role when the purpose is the determination of the physical laws that describe the process, rather than the prediction of its future behavior.

\citet{Stewart90SPT} approaches  classical matrix perturbation theory from a probabilistic point of view. The perturbed quantity is approximated by a first-order perturbation expansion, in which the perturbation is assumed to be random. This permits the computation of statistics estimating the variation in the perturbed quantity. Up to the higher-order terms that are ignored in the expansion, these statistics tend to be more realistic than perturbation bounds obtained in terms of norms. The technique is applied to a number of problems in matrix perturbation theory, including least squares and the eigenvalue problem.

\citet{Vaccaro94} develops a perturbation expansion for the subspace spanned by a set of singular vectors. A first-order expansion of this type has recently been developed and used to analyze the performance of direction-finding algorithms in array signal processing. The author derives a new second-order expansion and the result is illustrated with two examples.

\citet{VanHuffel91Book}  give a unified presentation of the TLS problem. A description of its basic principles are given, the various algebraic, statistical and sensitivity properties of the problem are discussed, and generalizations are presented. The TLS applications are surveyed to facilitate uses in an even wider range of applications. The authors provide comparison  with the well-known least squares methods.






\section{Literature relevant for Experimental validation state of the art}

\citet{Dienstfrey14} rederives the heuristic estimation rules that correct the measured quantities for the finite bandwidth of the measurement system response.
The rules are necessary when the measurement device responds slowly compared to the modulation of the input signal. The authors investigate the rules quantitative performance, and as an alternative, demonstrate that regularized deconvolution analysis exhibits more general quantitative utility at the expense of increased measurement burden and analytical complexity.

\citet{Eichstadt16b} explore the relationship between the covariance matrix produced by the Kalman filter, a GUM-compliant uncertainty analysis, and a Bayesian uncertainty analysis. The authors found that all three approaches are compatible for linear systems with known system matrices. However, different results are obtained when the system matrices are not precisely known, or when the system is nonlinear, which might use the extended Kalman filter with linearized uncertainty propagation. The paper shows that the extended Kalman filter linearization and convergence errors can be avoided by applying Monte Carlo.

\citet{Hammersley75} describes the theory and applications of  Monte Carlo Methods

\citet{Hessling10} Dynamic Metrology is devoted to the analysis of dynamic measurements.  As  an  extended  calibration  service,  it  contains  many  novel  ingredients currently not included in the standard palette of metrology. Rather, Dynamic Metrology encompasses many operations found in the fields of system identification, digital signal processing and control theory. The analyses are more complex and more ambiguous than conventional uncertainty budgets of today. The important interactions in non-stationary measurements  may  be  exceedingly  difficult  to  both  control  and  to  evaluate.  In  many situations, in situ calibrations are required to yield a relevant result. Providing metrological services in this context will be a true challenge

\citet{Hessling11} The time-dependent measurement uncertainty has been evaluated in a number of recent publications, starting from a known uncertain dynamic model. This could be defined as the 'downward' propagation of uncertainty from the model to the targeted measurement. The propagation of uncertainty 'upward' from the calibration experiment to a dynamic model traditionally belongs to system identification. The use of different representations (time, frequency, etc) is ubiquitous in dynamic measurement analyses. An expression of uncertainty in dynamic measurements is formulated for the first time in this paper independent of representation, joining upward as well as downward propagation. For applications in metrology, the high quality of the characterization may be prohibitive for any reasonably large and robust model to pass the whiteness test. This test is therefore relaxed by not directly requiring small systematic model errors in comparison to the randomness of the characterization. Instead, the systematic error of the dynamic model is propagated to the uncertainty of the measurand, analogously but differently to how stochastic contributions are propagated. The pass criterion of the model is thereby transferred from the identification to acceptance of the total accumulated uncertainty of the measurand. This increases the relevance of the test of the model as it relates to its final use rather than the quality of the calibration. The propagation of uncertainty hence includes the propagation of systematic model errors. For illustration, the 'upward' propagation of uncertainty is applied to determine if an appliance box is damaged in an earthquake experiment. In this case, relaxation of the whiteness test was required to reach a conclusive result.

\citet{Jafaripanah05}  investigate the application of analog adaptive techniques to the area of dynamic sensor compensation, of which there is little reported work in the literature. The case is illustrated by showing how the response of a load cell can be improved to speed up the process of measurement. The load cell is a sensor with an oscillatory response in which the measurand contributes to the response parameters. Thus, a compensation filter needs to track variation in measurand, whereas a simple fixed filter is only valid at one specific load value. To facilitate this investigation, computer models for the load cell and the adaptive compensation filter have been developed. To allow a practical implementation of the adaptive techniques, a novel piecewise linearization technique is proposed in order to vary a floating voltage-controlled resistor in a linear manner over a wide range. Simulation and practical results are presented, thus, demonstrating the effectiveness of the proposed techniques.

\citet{Ogorevc16}  describe a procedure for dynamic measurement uncertainty evaluation in order to examine the requirements for clinical thermometer dynamic properties in standards and recommendations. In this study thermistors were used as temperature sensors, transient temperature measurements were performed in water and air and the measurement data were processed for the investigation of thermometer dynamic properties. The thermometers were mathematically modelled. A Monte Carlo method was implemented for dynamic measurement uncertainty evaluation. The measurement uncertainty was analysed for static and dynamic conditions. Results showed that dynamic uncertainty is much larger than steady-state uncertainty. The results of dynamic uncertainty analysis were applied on an example of clinical measurements and were compared to current requirements in ISO standard for clinical thermometers. It can be concluded that there was no need for dynamic evaluation of clinical thermometers for continuous measurement, while dynamic measurement uncertainty was within the demands of target uncertainty. Whereas in the case of intermittent predictive thermometers, the thermometer dynamic properties had a significant impact on the measurement result. Estimation of dynamic uncertainty is crucial for the assurance of traceable and comparable measurements.
Clinical thermometers in intensive care units are used for the continuous measurement of body temperature. 




Use of load cell sensors

\citet{Olmi16} present a didactic project at the University of Bologna, focused on the development of strain gage load cells, from design to calibration and use. Details are provided on the course units involved and on the levels of teaching, and three case studies are presented. Two case studies deal with the design of decoupled load cells, using simple beam geometry. Students had the opportunity to become confident with the principles of mechanics and to tackle the not trivial activities like calibration by decoupled load application and determination of the related matrices. The third one deals with the development of a load cell for impact loads, where the students had the chance to observe a practical application of the studied Wheatstone bridge connection, to participate in on-field tests, and to better understand the close relationship between experimentation and design.
Many numerical models are nowadays available for the structural analysis of complexly shaped structures. However, a critical problem consists of the estimation of the actual loads that a structure withstands. The most proper way to determine them under service conditions consists of executing in-field tests, where load cells have an important role. This article deals

\citet{Piskorowski08} presents a new method for dynamic compensation of the load cell response using linear time-varying continuous-time filter. Load cells have an oscillatory response which always needs time to settle down. Therefore, it is justified to search a technique for an effective improvement of the sensor response. The paper describes a theoretical implementation of the proposed time-varying filter, and suggests the implementation technique with the aid of which this kind of filter can be implemented in practice. Simulation results verifying the effectiveness of the proposed filter are presented and compared to the traditional time-invariant configuration.

\citet{Rossander15} measures unique blade force on an open site straight-bladed vertical axis wind turbine. This paper presents a method for measuring the tangential and normal forces on a 12-kW vertical axis wind turbine prototype with a three-bladed H-rotor. Four single-axis load cells were installed in-between the hub and the support arms on one of the blades. The experimental setup, the measurement principle, together with the necessary control and measurement system are described. The maximum errors of the forces and accompanying weather data that can be obtained with the system are carefully estimated. Measured forces from the four load cells are presented, as well as the normal and tangential forces derived from them and a comparison with theoretical data. The measured torque and bending moment are also provided. The influence of the load cells on the turbine dynamics has also been evaluated. For the aerodynamic normal force, the system provides periodic data in agreement with simulations. Unexpected mechanical oscillations are present in the tangential force, introduced by the turbine dynamics. The measurement errors are of an acceptable size and often depend on the measured variable. Equations are presented for the calculation of measurement errors

\citet{Kesilmis16} We propose a novel geometric approach to processing of load cell signals for inline weight measurement applications. These systems usually rely on oscillatory load cell signals which always need to time to settle. We describe the theoretical and experimental implementation of a newly proposed geometric approach for load cell signals, and verify the effectiveness of the method through experiments.

\citet{Hernandez06} Load cells are transducers used to measure force or weight. Despite the fact that there is a wide variety of load cells, most of these transducers that are used in the weighingindustry are based on strain gauges. In this paper, an s-beam load cell based on strain gaugeswas suitably assembled to the mechanical structure of several seats of a bus under performance tests and used to measure the resistance of their mechanical structure to tension forces applied horizontally to the seats being tested. The load cell was buried in a broad-band noise background where the unwanted information and the relevant signal sometimes share a very similar frequency spectrum and its performance was improved by using arecursive least-squares (RLS) lattice algorithm. The experimental results are satisfactory and a significant improvement in the signal-to-noise ratio at the system output of 27 dB wasachieved, which is a good performance factor for judging the quality of the system.


\citet{Alaziz17} improve the during-sleep body motion classification using a load cell based system. With a machine learning algorithm, the authors classify a movement into one of 9 classes by extracting 24 features. The authors found that the method it classified correctly 90 \% of movements, and it is convenient for long-term home monitoring.

\citet{Ballo16}  present an innovative six axis load cell for frontal impact tests on a dummy.  The authors measure the acceleration in two different points of the dummy's head, and the impact force input. The authors explain the observations with a first order model of the head and neck, and the neck stiffness and damping have been identified from experimental data.

\citet{Boschetti13} propose a model-based scheme that compensates for low frequency environmental vibrations in automatic weighing machines made of load cells. The authors think that it is ineffective to use low-pass filtering only in load cell measurements to overcome this problem. Their argument is that low cut-off frequencies downgrade machine speed by both introducing delay and increasing filtered signal rise time. The authors instead use mechanical models of the weighing machine and the load cells to process supplementary accelerometer measurements for computing an effective compensation of the environmental vibrations on load cell response effects.

\citet{Guo18} A rule of the effect between the output of a single load cell and a combinatorial structure was proposed by changing the sensitivity of the single load cell. The relevant relationship between the sensitivity of single load cell and the variation of total output of the combinatorial structure was analyzed by the numerical derivation method and experimental validation. In the cases, the output of a single load cell is linear or nonlinear, when the same load was acted on the combinatorial structure. The results showed that the effect was limited and the influence quantity is due to the accuracy and number of load cell. However, it must be controlled in a certain range to avoid additional measurement errors to the combinatorial structure. Moreover, the rule provides an evaluation method with improving the measurement accuracy for the combinatorial structure. Consequently, the rule can be employed as a useful guidance for the product quality when this evaluation method is applied to the load cell selection for the electronic truck scales.


\citet{Zahradka18} demonstrates the feasibility of distinguishing certain positions and movements with load cell sensors. The paper shows that identification of positions and movements for individuals in bed can be used as a tool in a variety of clinical settings. Six positions were classified with an accuracy of 74.9 \% and six movements were classified with an accuracy of 79.7 \%. Stationary positions were characterized by the signals from the four load cells and the coordinates of the center of mass. Movements were characterized by the changes in load cell signals. 


Other types of sensors
\citet{DEmilia16} recognize the relevance of three-axis accelerometers and inclinometers in buildings diagnostics. The authors first focus on the sensors calibration, including the low frequency range, 0-10 Hz. Later, they minimize systematic errors and identify the factors that affect uncertainty: the motion positioning of the accelerometer with respect to the vertical axis, the imposed motion law, and the radial positioning of the sensor. 

\citet{Diniz17} present a methodology for estimating the uncertainty of measurements in the calibration of temperature sensors in dynamic regimes for industrial applications. The authors use a first-order bare-wire-beaded thermocouple sensor as a case study. The experimental setup is constructed to give the input as a temperature step. The paper describes a methodology to compute the measurement uncertainty considering the uncertainties of all components of the measurement chain.

\citet{Elster07} analyzes the dynamic measurements using a second-order accelerometer dynamic model, assuming that estimates of the model parameters are available after previous dynamic calibration measurements. The authors want to estimate the unknown input of the sensor given its discrete-time output signal, using an FIR filter constructed from the underlying dynamic model. The input  uncertainties are determined considering the uncertainty of the dynamic model parameters estimates, the uncertainty of the sensor's output signal, and the applied signal processing steps. The proposed method allows real-time estimations.

\citet{Link07} A recently proposed accelerometer model is applied for determining the accelerometer's output to transient accelerations. The model consists of a linear, second-order differential equation with unknown coefficients. It is proposed to estimate these model parameters from sinusoidal calibration measurements, and an estimation procedure based on linear least-squares is presented. In addition, the uncertainties associated with the estimated parameters are determined utilizing a Monte Carlo simulation technique. The performance of the proposed modelling approach was tested by its application to calibration measurements of two back-to-back accelerometers (ENDEVCO type 2270 and Br{\"u}el \& Kj{\ae}r type 8305). For each of the two accelerometers, the model was first estimated from sinusoidal calibration measurements and then used to predict the accelerometer's behaviour for two shock calibration measurements. Measured and predicted shock sensitivities were found consistent with differences below 1% in most cases which confirms the benefit of the proposed modelling approach.

\citet{Link09} propose a method  for evaluating the uncertainty associated with the output of a discrete-time IIR filter when the input signal is corrupted by additive noise and the filter coefficients are uncertain. This task arises, for instance, when the noise-corrupted output of a measurement system is compensated by a digital filter which has been designed on the basis of the characteristics of the measurement system. We assume that the noise is either stationary or uncorrelated, and we presume knowledge about its autocovariance function or its time-dependent variances, respectively. Uncertainty evaluation is considered in line with the 'Guide to the Expression of Uncertainty in Measurement'. A state-space representation is used to derive a calculation scheme which allows the uncertainties to be evaluated in an easy way and also enables real-time applications. The proposed procedure is illustrated by an example.

\citet{Markovsky15cep} Metrology is advancing by development of new measurement techniques and corresponding hardware. A given measurement technique, however, has fundamental speed and precision limitations. In order to overcome the hardware limitations, we develop signal processing methods based on the prior knowledge that the measurement process dynamics is linear time-invariant.
Our approach is to model the measurement process as a step response of a dynamical system, where the input step level is the quantity of interest. The solution proposed is an algorithm that does real-time processing of the sensor's measurements. It is shown that when the measurement process dynamics is known, the input estimation problem is equivalent to state estimation. Otherwise, the input estimation problem can be solved as a system identification problem. The main underlying assumption is that the measured quantity is constant and the measurement process is a low-order linear time-invariant system. The methods are validated and compared on applications of temperature and weight measurement.

\citet{Markovsky15ieee} Dynamic measurement aims to improve the speed and accuracy characteristics of measurement devices by signal processing. State-of-the-art dynamic measurement methods are model-based adaptive methods, i.e., 1) they estimate model parameters in real-time and 2) based on the identified model perform model-based signal processing. The proposed model-free method belongs to the class of the subspace identification methods. It computes directly the quantity of interest without an explicit parameter estimation. This allows efficient computation as well as applicability to general high order multivariable processes.


\medskip
\bibliography{../Gus-thesis/Gus-thesis.bib}


\end{document}
